# This example docker stack deployment file provides a complete mirror
# deployment stack based on simple local-storage based data backends for both
# swh-storage (using a postgresql backend) and swh-objstorage (using a
# pathslicer backend on a docker volume).
# !! WARNING!! This setup is NOT recommended for a production deployment.

x-swh-service: &swh-service
  image: softwareheritage/mirror:${SWH_IMAGE_TAG:-20260122-184240}

services:
  memcache:
    # used by the web app
    image: memcached:1.6
    deploy:
      replicas: 1

  kafka-ui:
    # give a nice UI to follow kafka consumers; also provides simple APIs used
    # by tests to introspect the kafka consumers' status
    image: ghcr.io/kafbat/kafka-ui
    environment:
      SPRING_CONFIG_ADDITIONAL-LOCATION: /config.yml
      SERVER_SERVLET_CONTEXT_PATH: /kafka-ui
    configs:
      - source: kafka-ui
        target: /config.yml

  storage-db:
    # Main storage (swh-storage) database.
    # ** TO BE MODIFIED **
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      # possible workaround to prevent dropped idle cnx (making pg pool fail to work after a while)
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.storage-db == true
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-storage
    volumes:
      - "storage-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-storage-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  masking-proxy-db:
    # Database for the masking proxy (see below)
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      # possible workaround to prevent dropped idle cnx (making pg pool fail to work after a while)
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.storage-db == true
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-masking-proxy
    volumes:
      - "masking-proxy-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-masking-proxy-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  web-db:
    # Database for the web application
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.web-db == true
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password-swh-web
      POSTGRES_USER: swh
      POSTGRES_DB: swh-web
    volumes:
      - "web-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-web-db-password
        target: postgres-password-swh-web
        uid: '999'
        mode: 0400

  web:
    # The web app (swh-web); serves both the web navigation interface and the
    # public web API; it is configured with a masking proxy (see below) as
    # swh-backend; allow to restrict public access to hidden/masked objects
    # (subject to Take Down Requests) and handle on-the-fly name change
    # corrections.
    <<: *swh-service
    configs:
      - source: web
        target: /etc/softwareheritage/config.yml
      - source: web-mirror-logo
        target: /etc/softwareheritage/mirror/static/logo_partner.png
      - source: web-mirror-footer
        target: /etc/softwareheritage/mirror/templates/mirror-footer.html
      - source: web-mirror-homepage
        target: /etc/softwareheritage/mirror/templates/mirror-homepage.html
    command: web
    environment:
      PGCFG_0: swh-web
      PGHOST_0: web-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-web
      PORT: "5004"
      DJANGO_SETTINGS_MODULE: swh.web.settings.production
    depends_on:
      - web-db
      - memcache
    secrets:
      - source: swh-mirror-web-db-password
        target: postgres-password-swh-web
        uid: '1000'
        mode: 0400

  sync-mailmap-cron:
    # cron-like job synchronizing mailmap declarations (web->storage)
    # Mailmap is the process allowinf to register personal identification
    # (name, email address) that should not be publicly displayed.
    <<: *swh-service
    # these config files should not be needed for this service, but currently,
    # if missing, the mailmap script fails to load properly...
    configs:
      - source: web
        target: /etc/softwareheritage/config.yml
      - source: web-mirror-logo
        target: /etc/softwareheritage/mirror/static/logo_partner.png
      - source: web-mirror-footer
        target: /etc/softwareheritage/mirror/templates/mirror-footer.html
      - source: web-mirror-homepage
        target: /etc/softwareheritage/mirror/templates/mirror-homepage.html
    command:
      - "django-admin"
      - "sync_mailmaps"
      - "--perform"
      - "service=swh-storage"
    environment:
      PGCFG_0: swh-web
      PGHOST_0: web-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-web
      PORT: "5004"
      DJANGO_SETTINGS_MODULE: swh.web.settings.production
    secrets:
      - source: swh-mirror-web-db-password
        target: postgres-password-swh-web
        uid: '1000'
        mode: 0400
    deploy:
      mode: replicated
      replicas: 0
      labels:
        - "swarm.cronjob.enable=true"
        - "swarm.cronjob.schedule=* * * * *"
        - "swarm.cronjob.skip-running=false"
      restart_policy:
        condition: none

  swarm-cronjob:
    # provides a docker-swarm-level cron-job-like feature; use to trigger
    # regular execution of the sync-mailmap-cron service (see below)
    image: crazymax/swarm-cronjob
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    environment:
      - "TZ=Europe/Paris"
      - "LOG_LEVEL=info"
      - "LOG_JSON=false"
    deploy:
      placement:
        constraints:
          - node.role == manager

  objstorage:
    # the swh-objstorage backend service; this example configuration uses a simple
    # filesystem-based pathslicing implementation of the swh-objstorage: see
    # https://docs.softwareheritage.org/devel/apidoc/swh.objstorage.backends.pathslicing.html
    <<: *swh-service
    deploy:
      # needed to allow actual and dynamic load balancing
      endpoint_mode: dnsrr
      # a real life replicas value better be in the 16 to 64 range
      replicas: 1
      placement:
        # note: if using a local volume, you need to pin the objstorage
        # instances on the node hosting the volume, eg. the manager, otherwise,
        # if using a remote/distributed objstorage backend (seaweedfs, cloud,
        # etc.) you want to remove this placement constraint
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.objstorage == true
    volumes:
      - "objstorage:/srv/softwareheritage/objects:rw,Z"
    configs:
      - source: objstorage
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      PORT: "5003"
    command: ["rpc-server", "objstorage"]

  storage:
    # the main (read-wrtie) swh-storage backend service; using postgresql
    # (storage-db) as backend
    <<: *swh-service
    deploy:
      # needed to allow actual and dynammic load balancing
      endpoint_mode: dnsrr
      # a real life replicas value better be in the 16 to 64 range
      # however we recommend keeping 1 in this stack deploy file so that
      # an upgrade of the base image that comes with a database migration script
      # is upgraded in a consistent way
      replicas: 1
    configs:
      - source: storage
        target: /etc/softwareheritage/config.yml
    environment:
      PGCFG_0: swh-storage
      PGHOST_0: storage-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-storage
      DB_FLAVOR: mirror
      PORT: "5002"
    env_file:
      - ./env/common-python.env
    secrets:
      - source: swh-mirror-storage-db-password
        target: postgres-password-swh-storage
        uid: '1000'
        mode: 0400
    command: ["rpc-server", "storage"]
    depends_on:
      - storage-db

  storage-public:
    # the (read-oly) swh-storage public backend service; comes with a masking
    # proxy before the actual storage allowing to hide objects after a TDN and
    # fix names according to recorded name changes
    <<: *swh-service
    deploy:
      # needed to allow actual and dynammic load balancing
      endpoint_mode: dnsrr
      # a real life replicas value better be in the 16 to 64 range
      # however we recommend keeping 1 in this stack deploy file so that
      # an upgrade of the base image that comes with a database migration script
      # is upgraded in a consistent way
      replicas: 1
    configs:
      - source: storage-public
        target: /etc/softwareheritage/config.yml
    environment:
      PGCFG_0: swh-masking-proxy
      PGHOST_0: masking-proxy-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-masking-proxy
      PORT: "5002"
    env_file:
      - ./env/common-python.env
    secrets:
      - source: swh-mirror-masking-proxy-db-password
        target: postgres-password-swh-masking-proxy
        uid: '1000'
        mode: 0400
    command: ["rpc-server", "storage"]

  nginx:
    # The main reverse proxy for all http services accessible from outside the
    # swarm cluster; serves the main web app, monitoring services (grafana,
    # prometheus) and several other services useful for investigating issues
    # etc. These should now be made publicly available. See conf/nginx.conf for
    # more details.
    image: nginx:1.25
    configs:
      - source: nginx
        target: /etc/nginx/nginx.conf
      - source: nginx-robots
        target: /usr/share/nginx/html/robots.txt
    ports:
      - "${SWH_PORT:-5081}:5081/tcp"
    # an actual deployment would probably set this:
    #deploy:
    #  mode: global

  prometheus:
    # the backend service for monitoring. This is the main time serties
    # database used by services to push metrics in (mostly via a statsd
    # collector, see below).
    image: prom/prometheus:v2.51.2
    depends_on:
    - prometheus-statsd-exporter
    command:
      # Needed for the reverse-proxy
      - "--web.external-url=/prometheus"
      - "--config.file=/etc/prometheus/prometheus.yml"
    configs:
      - source: prometheus
        target: /etc/prometheus/prometheus.yml
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.monitoring == true
    volumes:
      - "prometheus:/prometheus:rw,Z"

  ## uncomment this if you need to be able to push metrics from services not
  ## running in the swarm cluster, e.g. to allow a seaweedfs master to push metrics
  # prometheus-pushgateway:
  #   image: prom/pushgateway:v1.8.0
  #   ports:
  #     - 9091:9091

  prometheus-statsd-exporter:
    # The statsd->prometheus collector
    image: prom/statsd-exporter:v0.26.1
    command:
      - "--statsd.mapping-config=/etc/prometheus/statsd-mapping.yml"
    configs:
      - source: prometheus-statsd-exporter
        target: /etc/prometheus/statsd-mapping.yml

  grafana:
    # The graphical frontend used to expose prometheus metrics.
    image: grafana/grafana:10.4.2
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.monitoring == true
    depends_on:
    - prometheus
    environment:
      GF_SERVER_ROOT_URL: http://localhost:5081/grafana
    configs:
      - source: grafana-provisioning-datasources-prometheus
        target: /etc/grafana/provisioning/datasources/prometheus.yaml
      - source: grafana-provisioning-dashboards-all
        target: /etc/grafana/provisioning/dashboards/all.yaml
      - source: grafana-dashboards-backend-stats
        target: /var/lib/grafana/dashboards/backend-stats.json
      - source: grafana-dashboards-content-replayer
        target: /var/lib/grafana/dashboards/content-replayer.json
      - source: grafana-dashboards-graph-replayer
        target: /var/lib/grafana/dashboards/graph-replayer.json
    volumes:
      - "grafana:/var/lib/grafana:rw,Z"

## SWH Search services

  elasticsearch:
    # The elasticsearch database used as backend for search capabilities of the
    # archive.
    #
    # ** TO BE MODIFIED **
    # This is really just a minimal setup, not realistic for a production-like
    # environment
    image: elastic/elasticsearch:7.17.20
    deploy:
      resources:
        limits:
          memory: 2G
      placement:
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.elasticsearch == true
    env_file:
      - ./env/elasticsearch.env
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data

  search:
    # The search service (instance of swh-search) used by the main web app
    # (swh-web). It's a frontend to the elasticsearch above.
    <<: *swh-service
    env_file:
      - ./env/common-python.env
    environment:
      PORT: 5010
    configs:
      - source: search
        target: /etc/softwareheritage/config.yml
    command: ["rpc-server", "search"]

  search-journal-client-origin:
    # The replayer service responsible of filling the search service database
    # (elasticsearch) reading entries from the 'origin' kafka topic.
    <<: *swh-service
    env_file:
      - ./env/common-python.env
    configs:
      - source: search-journal-client
        target: /etc/softwareheritage/config.yml
    command: ["search-indexer", "-o", "origin"]

  search-journal-client-visit:
    # The replayer service responsible of filling the search service database
    # (elasticsearch) reading entries from the 'origin-visit' kafka topic.
    <<: *swh-service
    env_file:
      - ./env/common-python.env
    configs:
      - source: search-journal-client
        target: /etc/softwareheritage/config.yml
    command: ["search-indexer", "-o", "origin_visit_status"]


## replayer services

  redis:
    # Simple key-value store used to register replication issues occurring
    # during the replayer of the storage (swh-storage).
    image: redis:7.2
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.redis == true
    command:
      - 'redis-server'
      - '--save 60 1'
      - '--loglevel warning'
    volumes:
      - redis:/data

  graph-replayer:
    # The kafka replayer service responsible for the replication of the main
    # storage (swh-storage). This instance is handing all the kafka topics but
    # 'content' and 'dorectory' (see below). These 2 have dedicated replayer
    # services because they are significantly bigger than the other topics, and
    # required special tuning (especially scaling).
    <<: *swh-service
    deploy:
      # do not start replayers by default once the remaining of the stack is
      # running as expected, bump this value; expected real-life values should
      # be something in the range [16, 64] (staging) or [16, 256] (production)
      # depending on your hardware capabilities; note that there is no need of
      # going above the number of partitions on the kafka cluster (so the 64
      # and 254 upper limits depending on the execution environment).
      replicas: 0
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:graph-replayer,hostname:$${HOSTNAME}'
      SWH_LOG_LEVEL: WARNING
    configs:
      - source: graph-replayer
        target: /etc/softwareheritage/config.yml
    command:
      - "graph-replayer"
      - '--type=origin'
      - '--type=origin_visit'
      - '--type=origin_visit_status'
      - '--type=snapshot'
      - '--type=revision'
      - '--type=release'
      - '--type=skipped_content'
      - '--type=metadata_authority'
      - '--type=metadata_fetcher'
      - '--type=raw_extrinsic_metadata'
      - '--type=extid'
    depends_on:
      - storage
      - redis

  graph-replayer-content:
    # The kafka replayer service responsible for the replication of the main
    # storage (swh-storage). This instance is handing only the 'content' topic.
    <<: *swh-service
    deploy:
      # see above
      replicas: 0
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:graph-replayer,hostname:$${HOSTNAME}'
      SWH_LOG_LEVEL: WARNING
    configs:
      - source: graph-replayer-content
        target: /etc/softwareheritage/config.yml
    command:
      - "graph-replayer"
      - "--type=content"
    depends_on:
      - storage
      - redis

  graph-replayer-directory:
    # The kafka replayer service responsible for the replication of the main
    # storage (swh-storage). This instance is handing only the 'directory' topic.
    <<: *swh-service
    deploy:
      # see above
      replicas: 0
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:graph-replayer,hostname:$${HOSTNAME}'
      SWH_LOG_LEVEL: WARNING
    configs:
      - source: graph-replayer-directory
        target: /etc/softwareheritage/config.yml
    command:
      - "graph-replayer"
      - "--type=directory"
    depends_on:
      - storage
      - redis

  content-replayer:
    # The kafka replayer service responsible for the replication of the object
    # storage (swh-objstorage). Not to be confused with the
    # graph-replayer-content; this later fills the storage (swh-storage) with
    # content *descriptions* (hashes like sha1, sha256 etc, and size of the
    # content object) while this service is replicating the objects theselves,
    # copying them from the source objstorage (as configured in
    # cond/content-replayer.yml) to the local objstorage.
    <<: *swh-service
    deploy:
      # do not start replayers by default; see above
      replicas: 0
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:content-replayer,hostname:$${X_NODE_HOSTNAME}'
      SWH_LOG_LEVEL: 'INFO azure:ERROR'
    configs:
      - source: content-replayer
        target: /etc/softwareheritage/config.yml
    command:
      - content-replayer --concurrency=16
    depends_on:
      - objstorage
      - redis

## secondary services

  amqp:
    # used for the scheduling of Celery tasks involved in packing queried
    # directory or full repository history voa the vault service (see below).
    image: rabbitmq:3.13-management

### vault services

  vault-db:
    # the database for the vault rpc server
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.vault-db == true
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-vault
    volumes:
      - "vault-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-vault-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  vault:
    # The vault service.
    <<: *swh-service
    deploy:
      replicas: 1
    env_file:
      - ./env/common-python.env
    configs:
      - source: vault
        target: /etc/softwareheritage/config.yml
    environment:
      PGCFG_0: swh-vault
      PGHOST_0: vault-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-vault
      PORT: "5005"
      SWH_LOG_LEVEL: INFO
    command: ["rpc-server",  "vault"]
    secrets:
      - source: swh-mirror-vault-db-password
        target: postgres-password-swh-vault
        uid: '1000'
        mode: 0400

  vault-worker:
    # Celery worker dedicated to actually gather of the objects required to
    # build a (tar) archive for a requested directory (or git history). Since
    # this can be a very long process, it cannot be handled directly within the
    # web app; it is the jon of such and asynchronous backend task to do so and
    # motify the requester (email).
    <<: *swh-service
    deploy:
      replicas: 1
    env_file:
      - ./env/common-python.env
      - ./env/celery-worker.env
    environment:
      SWH_WORKER_INSTANCE: vault
      SWH_LOG_LEVEL: INFO
    configs:
      - source: vault-worker
        target: /etc/softwareheritage/config.yml
    command: celery-worker

  mailhog:
    # vault do really need someone to talk to via SMTP, so provide one for
    # testing purpose. This really need to be replaced by a proper SMTP
    # configuration for an actual deployment.
    image: mailhog/mailhog

### scheduler services

  scheduler-db:
    # database for the scheduler RPC server
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.scheduler-db == true
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-scheduler
    volumes:
      - "scheduler-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  scheduler:
    # Scheduler RPC service; used to schedule vault cooking tasks (asynchronous
    # build tar archive of a Directory or a git-bare file of the git history of
    # a repository).
    <<: *swh-service
    deploy:
      replicas: 1
    configs:
      - source: scheduler
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      PGCFG_0: swh-scheduler
      PGHOST_0: scheduler-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-scheduler
      PORT: "5008"
    command: ["rpc-server",  "scheduler"]
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password-swh-scheduler
        uid: '1000'
        mode: 0400

  scheduler-listener:
    # Helper service related to the task scheduling facility. This listens to
    # Celery event to keep the state of tasks on the scheduler database in sync
    # with their actual execution state.
    <<: *swh-service
    deploy:
      replicas: 1
    configs:
      - source: scheduler
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      SWH_WORKER_INSTANCE: scheduler
      PGCFG_0: swh-scheduler
      PGHOST_0: scheduler-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-scheduler
    command: ["scheduler", "start-listener"]
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password-swh-scheduler
        uid: '1000'
        mode: 0400

  scheduler-runner:
    # Helper service related to task scheduling facility. This regularly spawn
    # new Celery tasks according to scheduling policies.
    <<: *swh-service
    deploy:
      replicas: 1
    configs:
      - source: scheduler
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      SWH_WORKER_INSTANCE: scheduler
      PGCFG_0: swh-scheduler
      PGHOST_0: scheduler-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-scheduler
    command: ["scheduler", "start-runner", "--period", "10"]
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password-swh-scheduler
        uid: '1000'
        mode: 0400

  ## TDN propagation services
  notification-watcher:
    # Service dedicated to listen for alterations notififactions sent via a
    # dedicated kafka topic from the main SWH Archive. For each received
    # notification message, it will mask all the targeted objects (in the
    # masking-proxy-db) and send an email to listed operators so thath they can
    # decide what to do with the alteration request (apply it permanently or
    # reject it).
    <<: *swh-service
    deploy:
      replicas: 0  # do not start this at first, required for tests
    configs:
      - source: alter
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      SWH_LOG_LEVEL: DEBUG
      PGCFG_0: swh-masking-proxy
      PGHOST_0: masking-proxy-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-masking-proxy
    command: run-mirror-notification-watcher
    secrets:
      - source: swh-mirror-masking-proxy-db-password
        target: postgres-password-swh-masking-proxy
        uid: '1000'
        mode: 0400

    #volumes:
    #  - "./conf/mirror/age-identities.txt:/srv/softwareheritage/age-identities.txt:ro"


volumes:
  objstorage:
  redis:
  scheduler-db:
  storage-db:
  masking-proxy-db:
  vault-db:
  web-db:
  prometheus:
  grafana:
  elasticsearch-data:

secrets:
  swh-mirror-storage-db-password:
    external: true
  swh-mirror-masking-proxy-db-password:
    external: true
  swh-mirror-web-db-password:
    external: true
  swh-mirror-vault-db-password:
    external: true
  swh-mirror-scheduler-db-password:
    external: true

configs:
  alter:
    file: conf/alter.yml
  storage:
    file: conf/storage.yml
  storage-public:
    file: conf/storage-public.yml
  objstorage:
    file: conf/objstorage.yml
  nginx:
    file: conf/nginx.conf
  nginx-robots:
    file: conf/assets/nginx-robots.txt
  scheduler:
    file: conf/scheduler.yml
  vault:
    file: conf/vault.yml
  vault-worker:
    file: conf/vault-worker.yml
  web:
    file: conf/web.yml
  web-mirror-logo:
    file: conf/assets/logo_partner.png
  web-mirror-footer:
    file: conf/assets/mirror-footer.html
  web-mirror-homepage:
    file: conf/assets/mirror-homepage.html
  search:
    file: conf/search.yml
  search-journal-client:
    file: conf/search-journal-client.yml
  content-replayer:
    file: conf/content-replayer.yml
  graph-replayer:
    file: conf/graph-replayer.yml
  graph-replayer-content:
    file: conf/graph-replayer-content.yml
  graph-replayer-directory:
    file: conf/graph-replayer-directory.yml
  prometheus:
    file: conf/prometheus.yml
  prometheus-statsd-exporter:
    file: conf/prometheus-statsd-mapping.yml
  grafana-provisioning-datasources-prometheus:
    file: conf/grafana/provisioning/datasources/prometheus.yaml
  grafana-provisioning-dashboards-all:
    file: conf/grafana/provisioning/dashboards/all.yaml
  grafana-dashboards-graph-replayer:
    file: conf/grafana/dashboards/graph-replayer.json
  grafana-dashboards-content-replayer:
    file: conf/grafana/dashboards/content-replayer.json
  grafana-dashboards-backend-stats:
    file: conf/grafana/dashboards/backend-stats.json
  kafka-ui:
    file: conf/kafka-ui.yml


networks:
  default:
    driver: overlay
    attachable: true
    ipam:
      driver: default
      config:
        - subnet: ${SWH_SUBNET:-10.1.0.0/16}
