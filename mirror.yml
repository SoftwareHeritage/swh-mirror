version: "3.8"

services:
  memcache:
    # used by the web app
    image: memcached:1.6
    deploy:
      replicas: 1
    networks:
      - swh-mirror

  storage-db:
    # the main storage database
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      # possible workaround to prevent dropped idle cnx (making pg pool fail to work after a while)
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.storage-db == true
    networks:
      - swh-mirror
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-storage
    volumes:
      - "storage-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-storage-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  web-db:
    # the database for the web application
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.web-db == true
    networks:
      - swh-mirror
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password-swh-web
      POSTGRES_USER: swh
      POSTGRES_DB: swh-web
    volumes:
      - "web-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-web-db-password
        target: postgres-password-swh-web
        uid: '999'
        mode: 0400

  web:
    # the web app; serves both the web navigation interface and the public web API
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    networks:
      - swh-mirror
    configs:
      - source: web
        target: /etc/softwareheritage/config.yml
      - source: web-mirror-logo
        target: /etc/softwareheritage/mirror/static/logo_partner.png
      - source: web-mirror-footer
        target: /etc/softwareheritage/mirror/templates/mirror-footer.html
      - source: web-mirror-homepage
        target: /etc/softwareheritage/mirror/templates/mirror-homepage.html
    command: web
    environment:
      PGCFG_0: swh-web
      PGHOST_0: web-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-web
      PORT: "5004"
      DJANGO_SETTINGS_MODULE: swh.web.settings.production
    depends_on:
      - web-db
      - memcache
    secrets:
      - source: swh-mirror-web-db-password
        target: postgres-password-swh-web
        uid: '1000'
        mode: 0400

  objstorage:
    # the swh-objstorage backend service; this example configuration uses a simple
    # filesystem-based pathslicing implementation of the swh-objstorage: see
    # https://docs.softwareheritage.org/devel/apidoc/swh.objstorage.backends.pathslicing.html
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    deploy:
      # needed to allow actual and dynamic load balancing
      endpoint_mode: dnsrr
      # a real life replicas value better be in the 16 to 64 range
      replicas: 1
      placement:
        # note: if using a local volume, you need to pin the objstorage
        # instances on the node hosting the volume, eg. the manager, otherwise,
        # if using a remote/distributed objstorage backend (seaweedfs, cloud,
        # etc.) you want to remove this placement constraint
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.objstorage == true
    networks:
      - swh-mirror
    volumes:
      - "objstorage:/srv/softwareheritage/objects:rw,Z"
    configs:
      - source: objstorage
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      PORT: "5003"
    command: ["rpc-server", "objstorage"]

  storage:
    # the swh-storage backend service; using postgresql (storage-db) as backend
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    deploy:
      # needed to allow actual and dynammic load balancing
      endpoint_mode: dnsrr
      # a real life replicas value better be in the 16 to 64 range
      # however we recommend keeping 1 in this stack deploy file so that
      # an upgrade of the base image that comes with a database migration script
      # is upgraded in a consistent way
      replicas: 1
    networks:
      - swh-mirror
    configs:
      - source: storage
        target: /etc/softwareheritage/config.yml
    environment:
      PGCFG_0: swh-storage
      PGHOST_0: storage-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-storage
      DB_FLAVOR: mirror
      PORT: "5002"
    env_file:
      - ./env/common-python.env
    secrets:
      - source: swh-mirror-storage-db-password
        target: postgres-password-swh-storage
        uid: '1000'
        mode: 0400
    command: ["rpc-server", "storage"]
    depends_on:
      - storage-db

  nginx:
    image: nginx:1.25
    networks:
      - swh-mirror
    configs:
      - source: nginx
        target: /etc/nginx/nginx.conf
    ports:
      - "5081:5081/tcp"
    deploy:
      mode: global

  prometheus:
    image: prom/prometheus:v2.51.2
    networks:
      - swh-mirror
    depends_on:
    - prometheus-statsd-exporter
    command:
      # Needed for the reverse-proxy
      - "--web.external-url=/prometheus"
      - "--config.file=/etc/prometheus/prometheus.yml"
    configs:
      - source: prometheus
        target: /etc/prometheus/prometheus.yml
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.monitoring == true
    volumes:
      - "prometheus:/prometheus:rw,Z"

  ## uncomment this if you need to be able to push metrics from services not
  ## running in the swarm cluster, e.g. to allow a seaweedfs master to push metrics
  # prometheus-pushgateway:
  #   image: prom/pushgateway:v1.8.0
  #   networks:
  #     - swh-mirror
  #   ports:
  #     - 9091:9091

  prometheus-statsd-exporter:
    image: prom/statsd-exporter:v0.26.1
    networks:
      - swh-mirror
    command:
      - "--statsd.mapping-config=/etc/prometheus/statsd-mapping.yml"
    configs:
      - source: prometheus-statsd-exporter
        target: /etc/prometheus/statsd-mapping.yml

  grafana:
    image: grafana/grafana:10.4.2
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.monitoring == true
    networks:
      - swh-mirror
    depends_on:
    - prometheus
    environment:
      GF_SERVER_ROOT_URL: http://localhost:5081/grafana
    configs:
      - source: grafana-provisioning-datasources-prometheus
        target: /etc/grafana/provisioning/datasources/prometheus.yaml
      - source: grafana-provisioning-dashboards-all
        target: /etc/grafana/provisioning/dashboards/all.yaml
      - source: grafana-dashboards-backend-stats
        target: /var/lib/grafana/dashboards/backend-stats.json
      - source: grafana-dashboards-content-replayer
        target: /var/lib/grafana/dashboards/content-replayer.json
      - source: grafana-dashboards-graph-replayer
        target: /var/lib/grafana/dashboards/graph-replayer.json
    volumes:
      - "grafana:/var/lib/grafana:rw,Z"

## SWH Search services

  elasticsearch:
    image: elastic/elasticsearch:7.17.20
    deploy:
      placement:
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.elasticsearch == true
    networks:
      - swh-mirror
    env_file:
      - ./env/elasticsearch.env
    environment:
      - ingest.geoip.downloader.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data

  search:
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    networks:
      - swh-mirror
    env_file:
      - ./env/common-python.env
    environment:
      PORT: 5010
    configs:
      - source: search
        target: /etc/softwareheritage/config.yml
    command: ["rpc-server", "search"]

  search-journal-client:
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    networks:
      - swh-mirror
    env_file:
      - ./env/common-python.env
    configs:
      - source: search-journal-client
        target: /etc/softwareheritage/config.yml
    command: search-indexer


## replayer services

  redis:
    image: redis:7.2
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.redis == true
    networks:
      - swh-mirror
    command:
      - 'redis-server'
      - '--save 60 1'
      - '--loglevel warning'
    volumes:
      - redis:/data

  graph-replayer:
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    deploy:
      # do not start replayers by default once the remaining of the stack is
      # running as expected, bump this value; expected real-life values should
      # be something in the range [16, 64] (staging) or [16, 256] (production)
      # depending on your hardware capabilities; note that there is no need of
      # going above the number of partitions on the kafka cluster (so the 64
      # and 254 upper limits depending on the execution environment).
      replicas: 0
    networks:
      - swh-mirror
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:graph-replayer,hostname:$${HOSTNAME}'
    configs:
      - source: graph-replayer
        target: /etc/softwareheritage/config.yml
    command:
      - graph-replayer
    depends_on:
      - storage
      - redis

  content-replayer:
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    deploy:
      # do not start replayers by default; see above
      replicas: 0
    networks:
      - swh-mirror
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:content-replayer,hostname:$${X_NODE_HOSTNAME}'
      LOG_LEVEL: 'INFO --log-level azure:ERROR'
    configs:
      - source: content-replayer
        target: /etc/softwareheritage/config.yml
    command:
      - content-replayer --concurrency=16
    depends_on:
      - objstorage
      - redis

## secondary services

  amqp:
    image: rabbitmq:3.13-management
    networks:
      - swh-mirror

### vault services

  vault-db:
    # the database for the vault rpc server
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.vault-db == true
    networks:
      - swh-mirror
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-vault
    volumes:
      - "vault-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-vault-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  vault:
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    deploy:
      replicas: 1
    networks:
      - swh-mirror
    env_file:
      - ./env/common-python.env
    configs:
      - source: vault
        target: /etc/softwareheritage/config.yml
    environment:
      PGCFG_0: swh-vault
      PGHOST_0: vault-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-vault
      PORT: "5005"
      LOG_LEVEL: INFO
    command: ["rpc-server",  "vault"]
    secrets:
      - source: swh-mirror-vault-db-password
        target: postgres-password-swh-vault
        uid: '1000'
        mode: 0400

  vault-worker:
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    deploy:
      replicas: 1
    networks:
      - swh-mirror
    env_file:
      - ./env/common-python.env
      - ./env/celery-worker.env
    environment:
      SWH_WORKER_INSTANCE: vault
      LOG_LEVEL: INFO
    configs:
      - source: vault-worker
        target: /etc/softwareheritage/config.yml
    command: celery-worker

  # vault do really need someone to talk to via SMTP
  mailhog:
    image: mailhog/mailhog
    networks:
      - swh-mirror

### scheduler services

  scheduler-db:
    # the database for the vault rpc server
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.scheduler-db == true
    networks:
      - swh-mirror
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-scheduler
    volumes:
      - "scheduler-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  scheduler:
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    deploy:
      replicas: 1
    networks:
      - swh-mirror
    configs:
      - source: scheduler
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      PGCFG_0: swh-scheduler
      PGHOST_0: scheduler-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-scheduler
      PORT: "5008"
    command: ["rpc-server",  "scheduler"]
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password-swh-scheduler
        uid: '1000'
        mode: 0400

  scheduler-listener:
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    deploy:
      replicas: 1
    networks:
      - swh-mirror
    configs:
      - source: scheduler
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      SWH_WORKER_INSTANCE: scheduler
      PGCFG_0: swh-scheduler
      PGHOST_0: scheduler-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-scheduler
    command: ["scheduler", "start-listener"]
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password-swh-scheduler
        uid: '1000'
        mode: 0400

  scheduler-runner:
    image: softwareheritage/mirror:${SWH_IMAGE_TAG:-latest}
    deploy:
      replicas: 1
    networks:
      - swh-mirror
    configs:
      - source: scheduler
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      SWH_WORKER_INSTANCE: scheduler
      PGCFG_0: swh-scheduler
      PGHOST_0: scheduler-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-scheduler
    command: ["scheduler", "start-runner", "--period", "10"]
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password-swh-scheduler
        uid: '1000'
        mode: 0400


volumes:
  objstorage:
  redis:
  scheduler-db:
  storage-db:
  vault-db:
  web-db:
  prometheus:
  grafana:
  elasticsearch-data:

secrets:
  swh-mirror-storage-db-password:
    external: true
  swh-mirror-web-db-password:
    external: true
  swh-mirror-vault-db-password:
    external: true
  swh-mirror-scheduler-db-password:
    external: true

configs:
  storage:
    file: conf/storage.yml
  objstorage:
    file: conf/objstorage.yml
  nginx:
    file: conf/nginx.conf
  scheduler:
    file: conf/scheduler.yml
  vault:
    file: conf/vault.yml
  vault-worker:
    file: conf/vault-worker.yml
  web:
    file: conf/web.yml
  web-mirror-logo:
    file: conf/assets/logo_partner.png
  web-mirror-footer:
    file: conf/assets/mirror-footer.html
  web-mirror-homepage:
    file: conf/assets/mirror-homepage.html
  search:
    file: conf/search.yml
  search-journal-client:
    file: conf/search-journal-client.yml
  content-replayer:
    file: conf/content-replayer.yml
  graph-replayer:
    file: conf/graph-replayer.yml
  prometheus:
    file: conf/prometheus.yml
  prometheus-statsd-exporter:
    file: conf/prometheus-statsd-mapping.yml
  grafana-provisioning-datasources-prometheus:
    file: conf/grafana/provisioning/datasources/prometheus.yaml
  grafana-provisioning-dashboards-all:
    file: conf/grafana/provisioning/dashboards/all.yaml
  grafana-dashboards-graph-replayer:
    file: conf/grafana/dashboards/graph-replayer.json
  grafana-dashboards-content-replayer:
    file: conf/grafana/dashboards/content-replayer.json
  grafana-dashboards-backend-stats:
    file: conf/grafana/dashboards/backend-stats.json


networks:
  swh-mirror:
    driver: overlay
    attachable: true
    ipam:
      driver: default
      config:
        - subnet: 10.1.0.0/16
