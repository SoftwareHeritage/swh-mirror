# This example docker stack deployment file provides a complete mirror
# deployment stack based on more realistic data backends for both swh-storage
# (using a cassandra cluster) and swh-objstorage (using a winery backend)


x-swh-service: &swh-service
  image: softwareheritage/mirror:${SWH_IMAGE_TAG:-20251001-103527}

services:
  memcache:
    # used by the web app
    image: memcached:1.6
    deploy:
      replicas: 1

  kafka-ui:
    image: ghcr.io/kafbat/kafka-ui
    environment:
      SPRING_CONFIG_ADDITIONAL-LOCATION: /config.yml
      SERVER_SERVLET_CONTEXT_PATH: /kafka-ui
    configs:
      - source: kafka-ui
        target: /config.yml

  cassandra-seed:
    configs:
      - source: cassandra-override
        target: /cassandra-override.yml
      - source: cassandra-entrypoint
        target: /swh_entrypoint.sh
        mode: 0555
    entrypoint:
      - /swh_entrypoint.sh
    env_file:
      - ./env/cassandra.env
    environment:
      SERVICENAME: '{{.Service.Name}}'
    image: cassandra:5.0
    deploy:
      endpoint_mode: dnsrr

      # XXX deploying an actual cassandra cluster within the swarm prove to be
      # a bit difficult to do so reliably... Tests are very flaky when using 3
      # replicas for this service, so keep it easy for now with a single node
      # cassandra deployment.
      replicas: 1

      #placement:
      #  constraints:
      #    - node.role == manager

  masking-proxy-db:
    # the database for the masking proxy
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      # possible workaround to prevent dropped idle cnx (making pg pool fail to work after a while)
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.storage-db == true
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-masking-proxy
    volumes:
      - "masking-proxy-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-masking-proxy-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  web-db:
    # the database for the web application
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.web-db == true
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password-swh-web
      POSTGRES_USER: swh
      POSTGRES_DB: swh-web
    volumes:
      - "web-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-web-db-password
        target: postgres-password-swh-web
        uid: '999'
        mode: 0400

  web:
    # the web app; serves both the web navigation interface and the public web API
    <<: *swh-service
    configs:
      - source: web
        target: /etc/softwareheritage/config.yml
      - source: web-mirror-logo
        target: /etc/softwareheritage/mirror/static/logo_partner.png
      - source: web-mirror-footer
        target: /etc/softwareheritage/mirror/templates/mirror-footer.html
      - source: web-mirror-homepage
        target: /etc/softwareheritage/mirror/templates/mirror-homepage.html
    command: web
    environment:
      PGCFG_0: swh-web
      PGHOST_0: web-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-web
      PORT: "5004"
      DJANGO_SETTINGS_MODULE: swh.web.settings.production
    depends_on:
      - web-db
      - memcache
    secrets:
      - source: swh-mirror-web-db-password
        target: postgres-password-swh-web
        uid: '1000'
        mode: 0400

  sync-mailmap-cron:
    # cron-like job synchronizing mailmap declarations (web->storage)
    <<: *swh-service
    # these config files should not be needed for this service, but currently,
    # if missing, the mailmap script fails to load properly...
    configs:
      - source: web
        target: /etc/softwareheritage/config.yml
      - source: web-mirror-logo
        target: /etc/softwareheritage/mirror/static/logo_partner.png
      - source: web-mirror-footer
        target: /etc/softwareheritage/mirror/templates/mirror-footer.html
      - source: web-mirror-homepage
        target: /etc/softwareheritage/mirror/templates/mirror-homepage.html
    command:
      - "django-admin"
      - "sync_mailmaps"
      - "--perform"
      - "service=swh-storage"
    environment:
      PGCFG_0: swh-web
      PGHOST_0: web-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-web
      PORT: "5004"
      DJANGO_SETTINGS_MODULE: swh.web.settings.production
    secrets:
      - source: swh-mirror-web-db-password
        target: postgres-password-swh-web
        uid: '1000'
        mode: 0400
    deploy:
      mode: replicated
      replicas: 0
      labels:
        - "swarm.cronjob.enable=true"
        - "swarm.cronjob.schedule=* * * * *"
        - "swarm.cronjob.skip-running=false"
      restart_policy:
        condition: none

  swarm-cronjob:
    image: crazymax/swarm-cronjob
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    environment:
      - "TZ=Europe/Paris"
      - "LOG_LEVEL=info"
      - "LOG_JSON=false"
    deploy:
      placement:
        constraints:
          - node.role == manager

  winery-db:
    image: postgres:16
    deploy:
      placement:
        # note: if using a local volume, you need to pin the objstorage
        # instances on the node hosting the volume, eg. the manager, otherwise,
        # if using a remote/distributed objstorage backend (seaweedfs, cloud,
        # etc.) you want to remove this placement constraint
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.objstorage == true
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password-swh-winery
      POSTGRES_USER: swh
      POSTGRES_DB: swh-winery
    volumes:
      - "winery-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-winery-db-password
        target: postgres-password-swh-winery
        uid: '1000'
        mode: 0400

  winery-packer:
    <<: *swh-service
    deploy:
      placement:
        # note: if using a local volume, you need to pin the objstorage
        # instances on the node hosting the volume, eg. the manager, otherwise,
        # if using a remote/distributed objstorage backend (seaweedfs, cloud,
        # etc.) you want to remove this placement constraint
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.objstorage == true
    env_file:
      - ./env/common-python.env
    volumes:
      - "objstorage:/srv/softwareheritage/winery:rw,Z"
    configs:
      - source: objstorage
        target: /etc/softwareheritage/config.yml
    depends_on:
      - winery-db
      - objstorage  # wait for objstorage service to initialize the db
    environment:
      PGCFG_0: swh-winery
      PGHOST_0: winery-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-winery
    command: ["winery", "packer"]
    secrets:
      - source: swh-mirror-winery-db-password
        target: postgres-password-swh-winery
        uid: '1000'
        mode: 0400

  objstorage:
    # the swh-objstorage backend service; this example configuration uses a simple
    # filesystem-based pathslicing implementation of the swh-objstorage: see
    # https://docs.softwareheritage.org/devel/apidoc/swh.objstorage.backends.pathslicing.html
    <<: *swh-service
    deploy:
      # needed to allow actual and dynamic load balancing
      endpoint_mode: dnsrr
      # a real life replicas value better be in the 16 to 64 range
      replicas: 1
      placement:
        # note: if using a local volume, you need to pin the objstorage
        # instances on the node hosting the volume, eg. the manager, otherwise,
        # if using a remote/distributed objstorage backend (seaweedfs, cloud,
        # etc.) you want to remove this placement constraint
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.objstorage == true
    volumes:
      - "objstorage:/srv/softwareheritage/winery:rw,Z"
    configs:
      - source: objstorage
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      PGCFG_0: swh-winery
      PGHOST_0: winery-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-winery
      PORT: "5003"
      # we want to handle winery db init "by hand" for now, see the
      # entrypoint.sh script
      DB_SKIP_INIT: 1
      # NOT USING gunicorn threads is CRITICAL for winery
      GUNICORN_THREADS: 1
      # beware of this: 1 winery objstorage process = 1 active shard
      # so the total number of active shards will be 'replicas x workers'
      GUNICORN_WORKERS: 4

    command: ["rpc-server", "objstorage"]
    secrets:
      - source: swh-mirror-winery-db-password
        target: postgres-password-swh-winery
        uid: '1000'
        mode: 0400

  storage:
    # the swh-storage backend service; using cassandra as backend
    <<: *swh-service
    deploy:
      # needed to allow actual and dynammic load balancing
      endpoint_mode: dnsrr
      # a real life replicas value better be in the 16 to 64 range
      # however we recommend keeping 1 in this stack deploy file so that
      # an upgrade of the base image that comes with a database migration script
      # is upgraded in a consistent way
      replicas: 1
    configs:
      - source: storage-cassandra
        target: /etc/softwareheritage/config.yml
    environment:
      PORT: "5002"
      CASSANDRA_SEEDS: cassandra-seed
    env_file:
      - ./env/common-python.env
    command: ["rpc-server", "storage"]
    depends_on:
      - cassandra-seed

  storage-public:
    # the swh-storage public backend service; comes with a masking proxy before
    # the actual storage allowing to hide objects after a TDN
    <<: *swh-service
    deploy:
      # needed to allow actual and dynammic load balancing
      endpoint_mode: dnsrr
      # a real life replicas value better be in the 16 to 64 range
      # however we recommend keeping 1 in this stack deploy file so that
      # an upgrade of the base image that comes with a database migration script
      # is upgraded in a consistent way
      replicas: 1
    configs:
      - source: storage-public
        target: /etc/softwareheritage/config.yml
    environment:
      PGCFG_0: swh-masking-proxy
      PGHOST_0: masking-proxy-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-masking-proxy
      PORT: "5002"
    env_file:
      - ./env/common-python.env
    secrets:
      - source: swh-mirror-masking-proxy-db-password
        target: postgres-password-swh-masking-proxy
        uid: '1000'
        mode: 0400
    command: ["rpc-server", "storage"]

  nginx:
    image: nginx:1.25
    configs:
      - source: nginx
        target: /etc/nginx/nginx.conf
      - source: nginx-robots
        target: /usr/share/nginx/html/robots.txt
    ports:
      - "5081:5081/tcp"
    # an actual deployment would probably set this:
    #deploy:
    #  mode: global

  prometheus:
    image: prom/prometheus:v2.51.2
    depends_on:
    - prometheus-statsd-exporter
    command:
      # Needed for the reverse-proxy
      - "--web.external-url=/prometheus"
      - "--config.file=/etc/prometheus/prometheus.yml"
    configs:
      - source: prometheus
        target: /etc/prometheus/prometheus.yml
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.monitoring == true
    volumes:
      - "prometheus:/prometheus:rw,Z"

  ## uncomment this if you need to be able to push metrics from services not
  ## running in the swarm cluster, e.g. to allow a seaweedfs master to push metrics
  # prometheus-pushgateway:
  #   image: prom/pushgateway:v1.8.0
  #   ports:
  #     - 9091:9091

  prometheus-statsd-exporter:
    image: prom/statsd-exporter:v0.26.1
    command:
      - "--statsd.mapping-config=/etc/prometheus/statsd-mapping.yml"
    configs:
      - source: prometheus-statsd-exporter
        target: /etc/prometheus/statsd-mapping.yml

  grafana:
    image: grafana/grafana:10.4.2
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.monitoring == true
    depends_on:
    - prometheus
    environment:
      GF_SERVER_ROOT_URL: http://localhost:5081/grafana
    configs:
      - source: grafana-provisioning-datasources-prometheus
        target: /etc/grafana/provisioning/datasources/prometheus.yaml
      - source: grafana-provisioning-dashboards-all
        target: /etc/grafana/provisioning/dashboards/all.yaml
      - source: grafana-dashboards-backend-stats
        target: /var/lib/grafana/dashboards/backend-stats.json
      - source: grafana-dashboards-content-replayer
        target: /var/lib/grafana/dashboards/content-replayer.json
      - source: grafana-dashboards-graph-replayer
        target: /var/lib/grafana/dashboards/graph-replayer.json
    volumes:
      - "grafana:/var/lib/grafana:rw,Z"

## SWH Search services

  elasticsearch:
    # this is really just a minimal setup, not realistic for a production-like
    # environment
    image: elastic/elasticsearch:7.17.20
    deploy:
      resources:
        limits:
          memory: 2G
      placement:
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.elasticsearch == true
    env_file:
      - ./env/elasticsearch.env
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data

  search:
    <<: *swh-service
    env_file:
      - ./env/common-python.env
    environment:
      PORT: 5010
    configs:
      - source: search
        target: /etc/softwareheritage/config.yml
    command: ["rpc-server", "search"]

  search-journal-client-origin:
    <<: *swh-service
    env_file:
      - ./env/common-python.env
    configs:
      - source: search-journal-client
        target: /etc/softwareheritage/config.yml
    command: ["search-indexer", "-o", "origin"]

  search-journal-client-visit:
    <<: *swh-service
    env_file:
      - ./env/common-python.env
    configs:
      - source: search-journal-client
        target: /etc/softwareheritage/config.yml
    command: ["search-indexer", "-o", "origin_visit_status"]


## replayer services

  redis:
    image: redis:7.2
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.redis == true
    command:
      - 'redis-server'
      - '--save 60 1'
      - '--loglevel warning'
    volumes:
      - redis:/data

  graph-replayer:
    <<: *swh-service
    deploy:
      # do not start replayers by default once the remaining of the stack is
      # running as expected, bump this value; expected real-life values should
      # be something in the range [16, 64] (staging) or [16, 256] (production)
      # depending on your hardware capabilities; note that there is no need of
      # going above the number of partitions on the kafka cluster (so the 64
      # and 254 upper limits depending on the execution environment).
      replicas: 0
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:graph-replayer,hostname:$${HOSTNAME}'
      SWH_LOG_LEVEL: WARNING
    configs:
      - source: graph-replayer
        target: /etc/softwareheritage/config.yml
    command:
      - "graph-replayer"
      - '--type=origin'
      - '--type=origin_visit'
      - '--type=origin_visit_status'
      - '--type=snapshot'
      - '--type=revision'
      - '--type=release'
      - '--type=skipped_content'
      - '--type=metadata_authority'
      - '--type=metadata_fetcher'
      - '--type=raw_extrinsic_metadata'
      - '--type=extid'
    depends_on:
      - storage
      - redis

  graph-replayer-content:
    <<: *swh-service
    deploy:
      # see above
      replicas: 0
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:graph-replayer,hostname:$${HOSTNAME}'
      SWH_LOG_LEVEL: WARNING
    configs:
      - source: graph-replayer-content
        target: /etc/softwareheritage/config.yml
    command:
      - "graph-replayer"
      - "--type=content"
    depends_on:
      - storage
      - redis

  graph-replayer-directory:
    <<: *swh-service
    deploy:
      # see above
      replicas: 0
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:graph-replayer,hostname:$${HOSTNAME}'
      SWH_LOG_LEVEL: WARNING
    configs:
      - source: graph-replayer-directory
        target: /etc/softwareheritage/config.yml
    command:
      - "graph-replayer"
      - "--type=directory"
    depends_on:
      - storage
      - redis

  content-replayer:
    <<: *swh-service
    deploy:
      # do not start replayers by default; see above
      replicas: 0
    env_file:
      - ./env/common-python.env
    environment:
      STATSD_TAGS: 'role:content-replayer,hostname:$${X_NODE_HOSTNAME}'
      SWH_LOG_LEVEL: 'INFO azure:ERROR'
    configs:
      - source: content-replayer
        target: /etc/softwareheritage/config.yml
    command:
      - content-replayer --concurrency=16
    depends_on:
      - objstorage
      - redis

## secondary services

  amqp:
    image: rabbitmq:3.13-management

### vault services

  vault-db:
    # the database for the vault rpc server
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.vault-db == true
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-vault
    volumes:
      - "vault-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-vault-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  vault:
    <<: *swh-service
    deploy:
      replicas: 1
    env_file:
      - ./env/common-python.env
    configs:
      - source: vault
        target: /etc/softwareheritage/config.yml
    environment:
      PGCFG_0: swh-vault
      PGHOST_0: vault-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-vault
      PORT: "5005"
      SWH_LOG_LEVEL: INFO
    command: ["rpc-server",  "vault"]
    secrets:
      - source: swh-mirror-vault-db-password
        target: postgres-password-swh-vault
        uid: '1000'
        mode: 0400

  vault-worker:
    <<: *swh-service
    deploy:
      replicas: 1
    env_file:
      - ./env/common-python.env
      - ./env/celery-worker.env
    environment:
      SWH_WORKER_INSTANCE: vault
      SWH_LOG_LEVEL: INFO
    configs:
      - source: vault-worker
        target: /etc/softwareheritage/config.yml
    command: celery-worker

  # vault do really need someone to talk to via SMTP
  mailhog:
    image: mailhog/mailhog

### scheduler services

  scheduler-db:
    # the database for the vault rpc server
    image: postgres:16
    deploy:
      # we want only one replica of this service in the whole cluster
      replicas: 1
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.org.softwareheritage.mirror.volumes.scheduler-db == true
    command:
      - '--shared_buffers=4GB'
      - '--effective_cache_size=4GB'
      - '--random_page_cost=1.5'
      - '--max_wal_size=4GB'
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_USER: swh
      POSTGRES_DB: swh-scheduler
    volumes:
      - "scheduler-db:/var/lib/postgresql/data:rw,Z"
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password
        uid: '999'
        mode: 0400

  scheduler:
    <<: *swh-service
    deploy:
      replicas: 1
    configs:
      - source: scheduler
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      PGCFG_0: swh-scheduler
      PGHOST_0: scheduler-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-scheduler
      PORT: "5008"
    command: ["rpc-server",  "scheduler"]
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password-swh-scheduler
        uid: '1000'
        mode: 0400

  scheduler-listener:
    <<: *swh-service
    deploy:
      replicas: 1
    configs:
      - source: scheduler
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      SWH_WORKER_INSTANCE: scheduler
      PGCFG_0: swh-scheduler
      PGHOST_0: scheduler-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-scheduler
    command: ["scheduler", "start-listener"]
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password-swh-scheduler
        uid: '1000'
        mode: 0400

  scheduler-runner:
    <<: *swh-service
    deploy:
      replicas: 1
    configs:
      - source: scheduler
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      SWH_WORKER_INSTANCE: scheduler
      PGCFG_0: swh-scheduler
      PGHOST_0: scheduler-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-scheduler
    command: ["scheduler", "start-runner", "--period", "10"]
    secrets:
      - source: swh-mirror-scheduler-db-password
        target: postgres-password-swh-scheduler
        uid: '1000'
        mode: 0400

  ## TDN propagation services
  notification-watcher:
    <<: *swh-service
    deploy:
      replicas: 0  # do not start this at first, required for tests
    configs:
      - source: alter
        target: /etc/softwareheritage/config.yml
    env_file:
      - ./env/common-python.env
    environment:
      SWH_LOG_LEVEL: DEBUG
      PGCFG_0: swh-masking-proxy
      PGHOST_0: masking-proxy-db
      PGUSER_0: swh
      POSTGRES_DB_0: swh-masking-proxy
    command: run-mirror-notification-watcher
    secrets:
      - source: swh-mirror-masking-proxy-db-password
        target: postgres-password-swh-masking-proxy
        uid: '1000'
        mode: 0400

    #volumes:
    #  - "./conf/mirror/age-identities.txt:/srv/softwareheritage/age-identities.txt:ro"


volumes:
  objstorage:
  redis:
  scheduler-db:
  masking-proxy-db:
  vault-db:
  web-db:
  prometheus:
  grafana:
  elasticsearch-data:
  winery-db:

secrets:
  swh-mirror-masking-proxy-db-password:
    external: true
  swh-mirror-web-db-password:
    external: true
  swh-mirror-vault-db-password:
    external: true
  swh-mirror-scheduler-db-password:
    external: true
  swh-mirror-winery-db-password:
    external: true

configs:
  alter:
    file: conf/alter-cassandra.yml
  storage-cassandra:
    file: conf/storage-cassandra.yml
  storage-public:
    file: conf/storage-public.yml
  objstorage:
    file: conf/objstorage-winery.yml
  nginx:
    file: conf/nginx.conf
  nginx-robots:
    file: conf/assets/nginx-robots.txt
  scheduler:
    file: conf/scheduler.yml
  vault:
    file: conf/vault.yml
  vault-worker:
    file: conf/vault-worker.yml
  web:
    file: conf/web.yml
  web-mirror-logo:
    file: conf/assets/logo_partner.png
  web-mirror-footer:
    file: conf/assets/mirror-footer.html
  web-mirror-homepage:
    file: conf/assets/mirror-homepage.html
  search:
    file: conf/search.yml
  search-journal-client:
    file: conf/search-journal-client.yml
  content-replayer:
    file: conf/content-replayer.yml
  graph-replayer:
    file: conf/graph-replayer.yml
  graph-replayer-content:
    file: conf/graph-replayer-content.yml
  graph-replayer-directory:
    file: conf/graph-replayer-directory.yml
  prometheus:
    file: conf/prometheus.yml
  prometheus-statsd-exporter:
    file: conf/prometheus-statsd-mapping.yml
  grafana-provisioning-datasources-prometheus:
    file: conf/grafana/provisioning/datasources/prometheus.yaml
  grafana-provisioning-dashboards-all:
    file: conf/grafana/provisioning/dashboards/all.yaml
  grafana-dashboards-graph-replayer:
    file: conf/grafana/dashboards/graph-replayer.json
  grafana-dashboards-content-replayer:
    file: conf/grafana/dashboards/content-replayer.json
  grafana-dashboards-backend-stats:
    file: conf/grafana/dashboards/backend-stats.json
  kafka-ui:
    file: conf/kafka-ui.yml
  cassandra-entrypoint:
    file: conf/cassandra-swh-entrypoint.sh
  cassandra-override:
    file: conf/cassandra-override.yml

networks:
  default:
    driver: overlay
    attachable: true
    ipam:
      driver: default
      config:
        - subnet: 10.1.0.0/16
